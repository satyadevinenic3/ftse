{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc38cdc1-328c-4670-bdb3-eaf47fb49343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "OLD_ROOT = \"/data/TimeSeriesResearch/datasets/Satya\"\n",
    "NEW_ROOT = \"/data/TimeSeriesResearch/datasets/Satya_New\"\n",
    "\n",
    "def restructure_dataset(old_root=OLD_ROOT, new_root=NEW_ROOT):\n",
    "    os.makedirs(new_root, exist_ok=True)\n",
    "\n",
    "    for folder in tqdm(os.listdir(old_root), desc=\"Processing Folders\"):\n",
    "        folder_path = os.path.join(old_root, folder)\n",
    "        \n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            if not file_name.lower().endswith(\".csv\"):\n",
    "                continue\n",
    "            \n",
    "            parts = file_name.split('_')\n",
    "            if len(parts) >= 3:\n",
    "                label = '_'.join(parts[2:]).replace('.csv', '')\n",
    "                index = parts[1]\n",
    "            else:\n",
    "                label = \"Unknown\"\n",
    "                index = parts[1] if len(parts) > 1 else \"0\"\n",
    "\n",
    "            label_folder = os.path.join(new_root, folder, label)\n",
    "            os.makedirs(label_folder, exist_ok=True)\n",
    "\n",
    "            new_file_name = f\"file_{index}.csv\"\n",
    "            src_path = os.path.join(folder_path, file_name)\n",
    "            dst_path = os.path.join(label_folder, new_file_name)\n",
    "\n",
    "            try:\n",
    "                shutil.copy(src_path, dst_path)\n",
    "                print(f\"Moved: {src_path} -> {dst_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error moving {src_path} to {dst_path}: {e}\")\n",
    "\n",
    "    print(\"Restructuring complete.\")\n",
    "\n",
    "restructure_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44514e95-d098-41af-8bd5-b3bbf2200d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "ROOT_DIRS = [\n",
    "    '/data/TimeSeriesResearch/datasets/Satya_New'\n",
    "]\n",
    "\n",
    "H5_FILE = '/data/TimeSeriesResearch/datasets/satya_new_data1.h5'\n",
    "\n",
    "def create_h5py_dataset_from_root_dirs_v2(root_dirs=ROOT_DIRS, h5_file=H5_FILE):\n",
    "    with h5py.File(h5_file, \"w\") as h5f:\n",
    "        for root_dir in root_dirs:\n",
    "            root_prefix = os.path.basename(os.path.normpath(root_dir))\n",
    "\n",
    "            for folder in tqdm(os.listdir(root_dir), desc=f\"Processing {root_dir}\"):\n",
    "                folder_path = os.path.join(root_dir, folder)\n",
    "                if not os.path.isdir(folder_path):\n",
    "                    continue\n",
    "                #For labels\n",
    "                for subfolder in os.listdir(folder_path):\n",
    "                    subfolder_path = os.path.join(folder_path, subfolder)\n",
    "                    if not os.path.isdir(subfolder_path):\n",
    "                        continue\n",
    "\n",
    "                    for csv_file in os.listdir(subfolder_path):\n",
    "                        if not csv_file.lower().endswith(\".csv\"):\n",
    "                            continue\n",
    "\n",
    "                        try:\n",
    "                            group_path = f\"{root_prefix}/{folder}/{subfolder}\"\n",
    "                            group = h5f.require_group(group_path)\n",
    "\n",
    "                            csv_path = os.path.join(subfolder_path, csv_file)\n",
    "                            df = pd.read_csv(csv_path)\n",
    "\n",
    "                            drop_columns = ['Unnamed: 0', 'time_sec', 'Time', 'Label']\n",
    "                            df = df.drop(columns=[col for col in drop_columns if col in df.columns], errors='ignore')\n",
    "\n",
    "                            column_names = df.columns.tolist()\n",
    "                            data_array = df.to_numpy()\n",
    "                            dataset_name = f\"file_{len(group)}\"\n",
    "                            dset = group.create_dataset(dataset_name, data=data_array)\n",
    "                            dset.attrs[\"descriptions\"] = list(column_names)\n",
    "\n",
    "                            print(f\"Created dataset at: {group_path}/{dataset_name}\")\n",
    "\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing {csv_path}: {e}\")\n",
    "                            continue\n",
    "\n",
    "create_h5py_dataset_from_root_dirs_v2()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904be9ce-5c2d-4dc8-a61a-866678b6d425",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from ftse.data.Dataset import UnwindowedDataset\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "def load_h5py_dataset(h5_file=H5_FILE, window_size=None, stride=None, concat=False):\n",
    "    datasets = defaultdict(list)\n",
    "    def recursive_group_traversal(group, path=\"\"):\n",
    "        for key in group.keys():\n",
    "            item = group[key]\n",
    "\n",
    "            if isinstance(item, h5py.Group):\n",
    "                print(\"item: \", item)\n",
    "                recursive_group_traversal(item, path + \"/\" + key)\n",
    "            elif isinstance(item, h5py.Dataset):\n",
    "                label = path.split(\"/\")[-1]\n",
    "                dataset_name = \"/\".join(path.split(\"/\")[1:-1])\n",
    "\n",
    "                datasets[dataset_name].append(\n",
    "                    UnwindowedDataset(\n",
    "                        data=item,\n",
    "                        dataset_name=dataset_name,\n",
    "                        descriptions=item.attrs.get(\"descriptions\", []),\n",
    "                        label=label\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    f = h5py.File(h5_file, 'r')\n",
    "    recursive_group_traversal(f)\n",
    "\n",
    "    if window_size and stride:\n",
    "        for dataset_name in datasets:\n",
    "            datasets[dataset_name] = [\n",
    "                dataset.window(window_size=window_size, stride=stride)\n",
    "                for dataset in datasets[dataset_name]\n",
    "            ]\n",
    "\n",
    "    if concat:\n",
    "        for dataset_name in datasets:\n",
    "            datasets[dataset_name] = torch.utils.data.ConcatDataset(\n",
    "                datasets[dataset_name]\n",
    "            )\n",
    "\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6740a6a4-49bb-472e-8702-221d21749cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = load_h5py_dataset(H5_FILE, window_size=128, stride=1, concat=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf1c991-880a-4e2d-b687-4be4fab46910",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ftse",
   "language": "python",
   "name": "ftse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
